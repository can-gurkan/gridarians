"""
NetLogo Code Verifier Module

This module provides functionality to validate and verify NetLogo code generated by LLMs.
It ensures the code is syntactically correct, safe to execute, and follows best practices
for movement rules that agents use to explore the environment.

Dependencies:
- Python 3.8+
- Standard library modules: re, typing
"""

import re
from typing import List, Tuple, Set, Dict, Optional, Union, Pattern, Iterator, NamedTuple
from dataclasses import dataclass
from enum import Enum, auto
import logging

logger = logging.getLogger(__name__)


# --- Simple Type Representation ---
# Using constants for simplicity instead of a full Enum for now
TYPE_NUMBER = "number"
TYPE_STRING = "string"
TYPE_BOOLEAN = "boolean"
TYPE_LIST = "list"
TYPE_COMMAND_BLOCK = "command_block"
TYPE_AGENTSET = "agentset"
TYPE_AGENT = "agent"
TYPE_PATCH = "patch"
TYPE_LINK = "link"
TYPE_ANY = "any" # Could be anything (e.g., variable, complex reporter result)
TYPE_UNKNOWN = "unknown" # Parsing failed or type unclear
TYPE_INVALID = "invalid" # Represents an expression with known type errors


# --- Tokenizer Components ---

class TokenType(Enum):
    COMMAND = auto()
    REPORTER = auto()
    VARIABLE = auto()
    NUMBER = auto()
    STRING_LITERAL = auto()
    OPERATOR = auto()        # Arithmetic: +, -, *, /, ^
    STRING_CONCAT = auto()   # String concatenation: ++
    COMPARISON = auto()      # Comparison: =, !=, >, <, >=, <=
    LOGICAL = auto()         # Logical: and, or, not
    LPAREN = auto()          # (
    RPAREN = auto()          # )
    LBRACKET = auto()        # [
    RBRACKET = auto()        # ]
    COMMENT = auto()         # ; ...
    WHITESPACE = auto()      # Spaces, tabs
    NEWLINE = auto()         # \n
    IDENTIFIER = auto()      # General identifier before classification
    UNKNOWN = auto()         # Unrecognized token
    EOF = auto()             # End of File/Input

@dataclass
class Token:
    type: TokenType
    value: str
    line: int
    column: int

# --- End Tokenizer Components ---

class ErrorSeverity(Enum):
    """Severity levels for validation errors."""
    WARNING = "warning"
    ERROR = "error"

@dataclass
class ValidationError:
    """Class to represent validation errors with context."""
    message: str
    line_number: Optional[int] = None
    code_snippet: Optional[str] = None
    severity: ErrorSeverity = ErrorSeverity.ERROR

    def __str__(self) -> str:
        """String representation of the validation error."""
        location = f" at line {self.line_number}" if self.line_number is not None else ""
        snippet = f"\n  Code: '{self.code_snippet}'" if self.code_snippet else ""
        return f"{self.severity.value.upper()}{location}: {self.message}{snippet}"

@dataclass
class ValidationResult:
    """Result of a validation check."""
    is_valid: bool
    errors: List[ValidationError] = None

    def __init__(self, is_valid: bool, errors: Optional[List[ValidationError]] = None):
        self.is_valid = is_valid
        self.errors = errors or []

    def add_error(self, error: ValidationError) -> None:
        """Add an error to the result."""
        self.errors.append(error)
        self.is_valid = False

    def merge(self, other: 'ValidationResult') -> None:
        """Merge another validation result into this one."""
        self.is_valid = self.is_valid and other.is_valid
        self.errors.extend(other.errors)

class CodeComplexity(Enum):
    """Complexity levels for NetLogo code."""
    SIMPLE = 1      # Basic movement without conditions
    BASIC = 2       # Simple conditionals
    MODERATE = 3    # Multiple conditions, basic sensing
    ADVANCED = 4    # Complex conditions, environment awareness
    COMPLEX = 5     # Advanced sensing, memory usage
    SOPHISTICATED = 6  # Multiple strategies, adaptation
    EXPERT = 7      # Optimal pathfinding, complex decision making

class NetLogoVerifier:
    """
    NetLogo Code Verification and Validation Class

    This class provides comprehensive validation for NetLogo code generated by LLMs,
    focusing on verifying movement rules that agents use to explore environments.

    Public Methods:
    - is_safe(code: str) -> Tuple[bool, str]: Main validation method
    - validate(code: str) -> ValidationResult: Detailed validation with multiple errors
    - measure_complexity(code: str) -> CodeComplexity: Measures code complexity
    """
    def __init__(self, config: Optional[Dict] = None):
        """
        Initialize verifier with optional configuration.

        Args:
            config: Optional dictionary with configuration parameters
        """
        self.config = config or {}

        # Configure limits
        self.max_code_length = self.config.get("max_code_length", 10000)
        self.max_value = self.config.get("max_value", 1000)
        self.min_value = self.config.get("min_value", -1000)

        # Allowed NetLogo primitives
        self.allowed_commands = {
            # Movement commands
            'fd', 'forward',
            'rt', 'right',
            'lt', 'left',
            'bk', 'back',
            'stop',
            # Control structures
            'if', 'ifelse', 'ifelse-value',
            # Variable operations
            'set', 'let'

        }

        self.allowed_reporters = {
            # Random generators
            'random', 'random-float',
            # Math functions
            'sin', 'cos', 'tan', 'abs',
            # List operations
            'item', 'count', 'length', 'position', 'min', 'max',
            # Agent properties
            'xcor', 'ycor', 'heading',
            # Agent sensing
            'any?', 'in-radius', 'distance', 'towards',
            # Logic operators are handled by the parser, not as reporters
            # List/String constructors
            'list', 'word'
        }

        self.dangerous_primitives = {
            # Agent lifecycle
            'die', 'kill', 'create', 'hatch', 'sprout',
            # Agent control
            'ask', 'of', 'with',
            # Code execution
            'run', 'runresult',
            # File operations
            'file', 'import', 'export',
            # External code
            'python', 'js',
            # Simulation control
            'clear', 'reset', 'setup', 'go',
            # Loops
            'while', 'loop', 'repeat', 'forever',
            # Breeds
            'breed', 'create-ordered',
            # Network/extension operations
            'hubnet', 'gis', 'sql',
            # System operations
            'wait', 'beep', 'system',
            # Global state
            'clear-all', 'reset-ticks'
        }

        self.arithmetic_operators = {'+', '-', '*', '/', '^'}
        self.comparison_operators = {'=', '!=', '>', '<', '>=', '<='}
        self.allowed_variables = {
            'input', 'energy', 'lifetime', 'food-collected',
            'xcor', 'ycor', 'heading', 'who', 'input-resource-distances', 'input-resource-types',
            'food-observations', 'poison-observations', '"silver"', '"gold"', '"crystal"',
            'weight' # Added based on analysis of storeprompts.py
        }

        # Precompile regex patterns for performance
        self._compile_regex_patterns()
        # Compile tokenizer patterns
        self._compile_tokenizer_patterns()

    def _compile_regex_patterns(self) -> None:
        """Precompile regex patterns for validation (kept for now, may be deprecated)."""
        # Keep number pattern for value range checks (using regex temporarily)
        self.number_pattern = re.compile(r'[+-]?\d+(\.\d+)?([eE][+-]?\d+)?')
        # Old expression patterns removed
        self.comment_pattern = re.compile(r';.*$', re.MULTILINE) # Still useful

    def _compile_tokenizer_patterns(self) -> None:
        """Compile regex patterns for the tokenizer."""
        # Order matters! More specific patterns first.
        token_specs = [
            ('COMMENT',        r';[^\n]*'),                  # Comment until newline
            ('NEWLINE',        r'\n'),                       # Newline
            ('WHITESPACE',     r'[ \t]+'),                   # Whitespace (excluding newline)
            ('NUMBER',         r'[+-]?\d+(\.\d+)?([eE][+-]?\d+)?'), # Numbers (int, float, sci)
            ('STRING_LITERAL', r'"(?:\\.|[^"\\])*"'),        # String literal in double quotes
            ('LPAREN',         r'\('),                       # Left parenthesis
            ('RPAREN',         r'\)'),                       # Right parenthesis
            ('LBRACKET',       r'\['),                       # Left bracket
            ('RBRACKET',       r'\]'),                       # Right bracket
            ('COMPARISON',     r'!=|>=|<=|=|>|<'),           # Comparison operators
            ('STRING_CONCAT',  r'\+\+'),                     # String concat ++ (must come before single +)
            ('OPERATOR',       r'[+\-*/^]'),                 # Arithmetic operators (single char)
            # Identifier needs to be broad, classification happens later
            ('IDENTIFIER',     r'[a-zA-Z][\w\-]*\??'),       # Identifier (letters, digits, -, ?, starting with letter)
            ('UNKNOWN',        r'.'),                        # Any other character
        ]
        # Combine into a single regex for efficiency
        self.tokenizer_regex = re.compile(
            '|'.join(f'(?P<{name}>{pattern})' for name, pattern in token_specs)
        )

    def _tokenize(self, code: str) -> Iterator[Token]:
        """
        Generates a stream of tokens from the input NetLogo code string.

        Args:
            code: The raw NetLogo code string.

        Yields:
            Token objects representing the lexical elements of the code.
        """
        line_num = 1
        line_start = 0
        for mo in self.tokenizer_regex.finditer(code):
            kind = mo.lastgroup
            value = mo.group()
            column = mo.start() - line_start + 1

            if kind == 'NEWLINE':
                yield Token(TokenType.NEWLINE, value, line_num, column)
                line_num += 1
                line_start = mo.end()
            elif kind == 'WHITESPACE':
                pass # Ignore whitespace
            elif kind == 'COMMENT':
                pass # Ignore comments
            elif kind == 'UNKNOWN':
                yield Token(TokenType.UNKNOWN, value, line_num, column)
            else:
                token_type = TokenType[kind] # Map regex group name to Enum
                # Further classify IDENTIFIERs based on known lists
                if token_type == TokenType.IDENTIFIER:
                    val_lower = value.lower()
                    if val_lower in self.allowed_commands:
                        token_type = TokenType.COMMAND
                    elif val_lower in self.allowed_reporters:
                        token_type = TokenType.REPORTER
                    elif val_lower in {'and', 'or', 'not'}:
                         token_type = TokenType.LOGICAL
                    # Note: Variables remain IDENTIFIER unless matched above.

                yield Token(token_type, value, line_num, column)

        # Yield EOF token at the end
        yield Token(TokenType.EOF, '', line_num, len(code) - line_start + 1)


    def is_safe(self, code: str) -> Tuple[bool, str]:
        """
        Simplified interface to validate NetLogo code for safety and correctness.
        """
        result = self.validate(code)
        if not result.is_valid:
            return False, "\n".join(str(error) for error in result.errors)
        return True, "Code appears safe"

    def validate(self, code: str) -> ValidationResult:
        """
        Comprehensive validation of NetLogo code with detailed error reporting.
        """
        result = ValidationResult(True)

        # --- Step 1: Tokenization ---
        tokens = list(self._tokenize(code))
        filtered_tokens = [t for t in tokens if t.type not in {TokenType.WHITESPACE, TokenType.COMMENT, TokenType.NEWLINE} or t.type == TokenType.EOF]

        if not filtered_tokens or all(t.type == TokenType.EOF for t in filtered_tokens):
             result.add_error(ValidationError("Empty code or only comments/whitespace"))
             return result

        # Check for unknown tokens
        for token in filtered_tokens:
            if token.type == TokenType.UNKNOWN:
                result.add_error(ValidationError(
                    f"Unknown token: '{token.value}'",
                    line_number=token.line,
                    code_snippet=code.splitlines()[token.line-1][max(0, token.column-10):token.column+9]
                ))
        if not result.is_valid:
             return result

        # --- Step 2: Code Length Check ---
        if len(code) > self.max_code_length:
            result.add_error(ValidationError(f"Code exceeds maximum length of {self.max_code_length} characters"))

        # --- Step 3: Basic Structural Validation ---
        dangerous_result = self._check_dangerous_primitives_tokenized(filtered_tokens)
        result.merge(dangerous_result)
        if not result.is_valid: return result

        balance_result = self._check_brackets_balance_tokenized(filtered_tokens)
        result.merge(balance_result)
        if not result.is_valid: return result

        # --- Step 4: Detailed Validation ---
        movement_result = self._check_movement_commands_tokenized(filtered_tokens)
        result.merge(movement_result)

        syntax_result = self._check_syntax_tokenized(filtered_tokens)
        result.merge(syntax_result)
        # Stop early if major syntax errors occurred before checking ranges
        if not result.is_valid: return result

        # Call the token-based value range check
        range_result = self._check_value_ranges(filtered_tokens)
        result.merge(range_result)

        return result

    def _check_dangerous_primitives_tokenized(self, tokens: List[Token]) -> ValidationResult:
        """Validate against dangerous primitives using tokens."""
        result = ValidationResult(True)
        for token in tokens:
            # Check commands, reporters, and general identifiers that might match
            if token.type in {TokenType.COMMAND, TokenType.REPORTER, TokenType.IDENTIFIER}:
                if token.value.lower() in self.dangerous_primitives:
                    result.add_error(ValidationError(
                        f"Dangerous primitive found: {token.value}",
                        line_number=token.line,
                        code_snippet=token.value
                    ))
        return result

    def _check_brackets_balance_tokenized(self, tokens: List[Token]) -> ValidationResult:
        """Validate bracket/parenthesis balance using tokens."""
        result = ValidationResult(True)
        stack = []
        bracket_map = {TokenType.LPAREN: TokenType.RPAREN, TokenType.LBRACKET: TokenType.RBRACKET}
        opening_types = {TokenType.LPAREN, TokenType.LBRACKET}
        closing_types = {TokenType.RPAREN, TokenType.RBRACKET}

        for token in tokens:
            if token.type in opening_types:
                stack.append((bracket_map[token.type], token))
            elif token.type in closing_types:
                if not stack:
                    result.add_error(ValidationError(
                        f"Unmatched closing bracket/parenthesis: '{token.value}'",
                        line_number=token.line, code_snippet=token.value
                    ))
                else:
                    expected_type, opening_token = stack.pop()
                    if token.type != expected_type:
                        result.add_error(ValidationError(
                            f"Mismatched bracket/parenthesis: Expected closing for '{opening_token.value}' (line {opening_token.line}) but found '{token.value}'",
                            line_number=token.line, code_snippet=f"...{opening_token.value}...{token.value}..."
                        ))

        for _, opening_token in stack:
             result.add_error(ValidationError(
                 f"Unclosed bracket/parenthesis: '{opening_token.value}'",
                 line_number=opening_token.line, code_snippet=opening_token.value
             ))
        return result

    def _check_movement_commands_tokenized(self, tokens: List[Token]) -> ValidationResult:
        """Verify presence of at least one movement command using tokens."""
        result = ValidationResult(True)
        has_ifelse_value = any(t.type == TokenType.COMMAND and t.value.lower() == 'ifelse-value' for t in tokens)
        if has_ifelse_value:
            return result # ifelse-value doesn't require movement commands

        movement_commands = {'fd', 'forward', 'rt', 'right', 'lt', 'left', 'bk', 'back'}
        found_movement = any(t.type == TokenType.COMMAND and t.value.lower() in movement_commands for t in tokens)

        if not found_movement:
            result.add_error(ValidationError(
                "No movement commands found. Code must include at least one movement command: fd, rt, lt, or bk",
                severity=ErrorSeverity.ERROR
            ))
        return result

    def _check_syntax_tokenized(self, tokens: List[Token]) -> ValidationResult:
        """Validate overall syntax using tokens (main entry point)."""
        result = ValidationResult(True)
        i = 0
        while i < len(tokens) and tokens[i].type != TokenType.EOF:
            token = tokens[i]
            consumed_count = 1 # Default consumption

            if token.type == TokenType.COMMAND:
                if token.value.lower() in {'if', 'ifelse', 'ifelse-value'}:
                    if_result, consumed_count = self._validate_if_statement(tokens, i)
                    result.merge(if_result)
                elif token.value.lower() in self.allowed_commands:
                    cmd_result, consumed_count = self._validate_command(tokens, i)
                    result.merge(cmd_result)
                else:
                    # Should have been caught by dangerous check, but safeguard
                    result.add_error(ValidationError(f"Unexpected command: {token.value}", line_number=token.line))
            elif token.type in {TokenType.RPAREN, TokenType.RBRACKET}:
                 # Closing brackets/parens shouldn't appear at the top level
                 result.add_error(ValidationError(f"Unexpected closing token: '{token.value}'", line_number=token.line))
            # Handle multi-conditional ifelse starting with '('
            elif token.type == TokenType.LPAREN:
                 # Peek ahead: Expect 'ifelse' or 'ifelse-value' command next
                 if i + 1 < len(tokens) and tokens[i+1].type == TokenType.COMMAND and tokens[i+1].value.lower() in {'ifelse', 'ifelse-value'}:
                      statement_type = tokens[i+1].value.lower()
                      # Call the multi-conditional validator starting from the LPAREN
                      multi_cond_result, consumed_count = self._validate_multi_conditional(tokens, i, statement_type)
                      result.merge(multi_cond_result)
                 else:
                      # Parenthesized expression not allowed at top level, or invalid multi-conditional start
                      peek_token_desc = f"'{tokens[i+1].value}' ({tokens[i+1].type.name})" if i + 1 < len(tokens) else "end of input"
                      result.add_error(ValidationError(f"Unexpected parenthesis '(' at top level, or invalid start to multi-conditional ifelse (found {peek_token_desc} after '(')", line_number=token.line))
                      # Try to consume until matching RPAREN for basic recovery
                      paren_level = 1
                      consumed_count = 1
                      temp_i = i + 1
                      while temp_i < len(tokens):
                           consumed_count += 1
                           if tokens[temp_i].type == TokenType.LPAREN: paren_level += 1
                           elif tokens[temp_i].type == TokenType.RPAREN:
                               paren_level -= 1
                               if paren_level == 0: break
                           elif tokens[temp_i].type == TokenType.EOF: break
                           temp_i += 1
                      if paren_level != 0:
                           result.add_error(ValidationError(f"Unclosed parenthesis starting on line {token.line}", line_number=token.line))


            # Other tokens are unexpected at the top level
            else:
                 # Allow expressions at the top level ONLY if they are the entire content
                 # (e.g., a single reporter call like `random 10`) - this is unusual but possible
                 if i == 0:
                      expr_result, consumed_count, _ = self._validate_expression(tokens, i, min_precedence=-1)
                      if not expr_result.is_valid:
                           result.merge(expr_result)
                           # If the expression itself failed, report that
                      elif consumed_count < len(tokens) -1: # Check if it consumed all non-EOF tokens
                           result.add_error(ValidationError(f"Unexpected token after expression: {tokens[consumed_count].type.name} ('{tokens[consumed_count].value}')", line_number=tokens[consumed_count].line))
                           consumed_count = 1 # Reset consumption on error
                      # If valid and consumed all, it's okay (though maybe warn?)
                 else:
                      result.add_error(ValidationError(f"Unexpected token at top level: {token.type.name} ('{token.value}')", line_number=token.line))
                      consumed_count = 1 # Consume the unexpected token

            i += consumed_count # Advance by the number of tokens consumed by the validator
            if not result.is_valid and consumed_count > 0 : # Check consumed_count > 0 to avoid infinite loop if validator returns 0
                 # If a top-level command failed validation, stop further checks at this level
                 # to avoid cascading errors from a single malformed command.
                 # However, allow checking subsequent independent commands.
                 # Consider if we should stop entirely on first error. For now, continue.
                 pass

        return result

    # --- Expression Validator (Pratt Parser Style with Basic Type Inference) ---

    # Operator precedence levels (higher value = higher precedence)
    OPERATOR_PRECEDENCE = {
        # Arithmetic (PEMDAS/BODMAS like)
        '^': 4, # Exponentiation (Right-associative, handle later if needed)
        '*': 3, '/': 3, # Multiplication/Division
        '+': 2, '-': 2, # Addition/Subtraction
        # String concat - assign precedence, e.g., same as addition
        '++': 2,
        # Comparison
        '=': 1, '!=': 1, '>': 1, '<': 1, '>=': 1, '<=': 1,
        # Logical
        'not': 5, # Unary logical NOT (high precedence)
        'and': 0,
        'or': -1, # Lowest precedence
    }

    # Define expected number of arguments for reporters (keep simple for now)
    # TODO: Expand this with return types later if needed for more complex checks
    REPORTER_ARITY = {
        # Random
        'random': 1, 'random-float': 1,
        # Math
        'sin': 1, 'cos': 1, 'tan': 1,
        # List (Note: 'item' takes list and index)
        'item': 2, 'count': 1, 'length': 1, 'position': 2,
        # Agent properties (zero args)
        'xcor': 0, 'ycor': 0, 'heading': 0,
        # Agent Sensing (Treat agentsets/targets as single expressions for now)
        'any?': 1,
        'in-radius': 2, # agentset, radius
        'distance': 1, # target
        'towards': 1, # target
        'min': 2, # number, number
        # Logical operators 'and', 'or', 'not' are handled by precedence parsing.
        # List/String constructors
        'list': -1, # Variadic arity
        'word': -1, # Variadic arity
    }


    def _get_token_precedence(self, token: Optional[Token]) -> int:
        """Returns the precedence of an infix operator token, or -2 if not an infix operator."""
        if token is None:
            return -2
        op_value = token.value.lower()
        # Check arithmetic/comparison operators first
        if token.type in {TokenType.OPERATOR, TokenType.COMPARISON}:
            return self.OPERATOR_PRECEDENCE.get(op_value, -2)
        # Check logical operators (only 'and', 'or' are infix)
        if token.type == TokenType.LOGICAL and op_value in {'and', 'or'}:
             return self.OPERATOR_PRECEDENCE.get(op_value, -2)
        # Check string concat
        if token.type == TokenType.STRING_CONCAT:
             # Use token.value directly for '++' as it's not lowercased in OPERATOR_PRECEDENCE
             return self.OPERATOR_PRECEDENCE.get(token.value, -2)
        return -2 # Not a recognized infix operator

    def _can_start_expression(self, token: Optional[Token]) -> bool:
        """Checks if a token type can potentially start a valid expression."""
        if token is None:
            return False
        # Primary terms
        if token.type in {TokenType.NUMBER, TokenType.IDENTIFIER, TokenType.STRING_LITERAL,
                          TokenType.LPAREN, TokenType.REPORTER, TokenType.LBRACKET}: # Added LBRACKET
            return True
        # Prefix operators (unary +/-, not)
        if token.type == TokenType.OPERATOR and token.value in {'+', '-'}:
            return True
        if token.type == TokenType.LOGICAL and token.value.lower() == 'not':
            return True
        # String concat is infix only
        return False

    def _parse_prefix_or_primary(self, tokens: List[Token], start_index: int) -> Tuple[ValidationResult, int, str]:
        """
        Parses prefix operators (unary -, +, not) and primary expression terms
        (numbers, variables, strings, parenthesized expressions, reporter calls, command blocks).
        Returns the validation result, the index after the parsed term, and the inferred type string.

        Args:
            tokens: The list of tokens.
            start_index: The index in the token list where the prefix/primary starts.

        Returns:
            A tuple containing:
            - ValidationResult: Indicates if the parsed prefix/primary is valid.
            - int: The index of the token immediately *after* the parsed part.
            - str: The inferred type string (e.g., TYPE_NUMBER, TYPE_INVALID).
        """
        result = ValidationResult(True)
        i = start_index
        inferred_type = TYPE_UNKNOWN # Default type

        if i >= len(tokens) or tokens[i].type == TokenType.EOF:
            result.add_error(ValidationError(
                "Expected expression term or prefix operator, found end of input",
                line_number=tokens[i-1].line if i > 0 else 1
            ))
            return result, i, TYPE_INVALID

        current_token = tokens[i]
        next_index = i + 1 # Default consumption is 1 token

        # --- Handle Prefix Operators ---
        # Check for unary minus/plus (distinct from binary operators)
        if current_token.type == TokenType.OPERATOR and current_token.value in {'+', '-'}:
            # Treat as unary prefix operator
            op_token = current_token
            # Recursively parse the operand that follows the unary operator.
            # Use high precedence (e.g., 6, higher than 'not') to bind tightly.
            # We only need the structure and index here, type check happens in _validate_expression
            operand_result, operand_end_index, _ = self._validate_expression(tokens, i + 1, min_precedence=6) # Ignore operand type for now
            result.merge(operand_result)
            next_index = operand_end_index

            # Assume unary +/- result in a number if the operand was structurally valid
            # More detailed type checking will happen in the caller (_validate_expression)
            inferred_type = TYPE_NUMBER if operand_result.is_valid else TYPE_INVALID

            return result, next_index, inferred_type

        # Check for logical 'not'
        elif current_token.type == TokenType.LOGICAL and current_token.value.lower() == 'not':
            op_token = current_token
            not_precedence = self.OPERATOR_PRECEDENCE.get('not', 5)
            operand_result, operand_end_index, _ = self._validate_expression(tokens, i + 1, min_precedence=not_precedence) # Ignore operand type
            result.merge(operand_result)
            next_index = operand_end_index

            # Assume 'not' results in a boolean if the operand was structurally valid
            inferred_type = TYPE_BOOLEAN if operand_result.is_valid else TYPE_INVALID

            return result, next_index, inferred_type


        # --- Handle Primary Terms ---
        elif current_token.type == TokenType.NUMBER:
            inferred_type = TYPE_NUMBER
        elif current_token.type == TokenType.IDENTIFIER:
            var_name = current_token.value.lower()
            if var_name not in self.allowed_variables:
                 # Check if it's a known 0-arity reporter like 'xcor'
                 arity = self.REPORTER_ARITY.get(var_name)
                 if arity == 0:
                      # Basic type inference for known 0-arity reporters
                      if var_name in {'xcor', 'ycor', 'heading', 'random', 'random-float'}: # Add more
                           inferred_type = TYPE_NUMBER
                      # elif var_name in {'any?'}: # Example boolean reporter (needs arity 1, error?) -> This logic needs refinement
                      #      # This case should likely be handled by the REPORTER block below if arity > 0
                      #      # If arity is 0, it must be a simple property like xcor
                      #      result.add_error(ValidationError(f"Reporter '{var_name}' used without arguments, but expects arguments.", line_number=current_token.line))
                      #      inferred_type = TYPE_INVALID
                      else:
                           inferred_type = TYPE_ANY # Assume unknown 0-arity reporters return anything
                 elif arity is not None: # It's a known reporter but used incorrectly (without args)
                      result.add_error(ValidationError(f"Reporter '{var_name}' used without arguments, but expects {arity} argument(s).", line_number=current_token.line))
                      inferred_type = TYPE_INVALID
                 else: # Not an allowed variable or known reporter
                      result.add_error(ValidationError(
                          f"Unknown or disallowed identifier/variable: '{current_token.value}'",
                          line_number=current_token.line, code_snippet=current_token.value
                      ))
                      inferred_type = TYPE_INVALID
            else:
                 # Basic type inference for known variables
                 if var_name in {'xcor', 'ycor', 'heading', 'who', 'energy', 'lifetime', 'food-collected', 'weight'}:
                      inferred_type = TYPE_NUMBER
                 elif var_name in {'"silver"', '"gold"', '"crystal"'}:
                      inferred_type = TYPE_STRING
                 elif var_name in {'input-resource-distances', 'input-resource-types', 'food-observations', 'poison-observations'}:
                      inferred_type = TYPE_LIST
                 else:
                      inferred_type = TYPE_ANY # Assume unknown allowed variables can be any type
        elif current_token.type == TokenType.STRING_LITERAL:
             inferred_type = TYPE_STRING

        # --- Handle Parentheses OR Parenthesized List/Word Constructor ---
        elif current_token.type == TokenType.LPAREN:
            paren_token = current_token
            # Check for specific parenthesized reporters like (list ...) or (word ...)
            is_special_reporter = False
            if i + 1 < len(tokens) and tokens[i+1].type == TokenType.REPORTER:
                 reporter_name = tokens[i+1].value.lower()
                 arity = self.REPORTER_ARITY.get(reporter_name)
                 # Check if it's a known variadic reporter (-1 arity)
                 if arity == -1:
                      is_special_reporter = True
                      i += 2 # Consume '(' and reporter name
                      # Loop to parse zero or more arguments
                      while True:
                           if i >= len(tokens) or tokens[i].type == TokenType.EOF:
                               result.add_error(ValidationError(f"Expected ')' to close '({reporter_name} ...)' opened on line {paren_token.line}, found end of input", line_number=paren_token.line))
                               next_index = i
                               return result, next_index, TYPE_INVALID # Stop processing

                           # Check for closing parenthesis
                           if tokens[i].type == TokenType.RPAREN:
                               break # End of list arguments

                           # Parse the next argument expression, ignore its type for now
                           arg_result, arg_end_index, _ = self._validate_expression(tokens, i, min_precedence=-1)
                           result.merge(arg_result)

                           if not arg_result.is_valid:
                                # Add context if needed
                                if result.errors and f"'({reporter_name} ...)'" not in result.errors[-1].message:
                                     # Find last error and add context
                                     for err in reversed(result.errors):
                                          if f"'({reporter_name} ...)'" not in err.message:
                                               err.message = f"Invalid argument within '({reporter_name} ...)' starting on line {paren_token.line}: {err.message}"
                                               break
                                # Attempt recovery
                                temp_i = arg_end_index
                                paren_level = 1
                                while temp_i < len(tokens):
                                     if tokens[temp_i].type == TokenType.LPAREN: paren_level += 1
                                     elif tokens[temp_i].type == TokenType.RPAREN:
                                         paren_level -= 1
                                         if paren_level == 0: break
                                     elif tokens[temp_i].type == TokenType.EOF: break
                                     temp_i += 1
                                next_index = temp_i + 1 if temp_i < len(tokens) and tokens[temp_i].type == TokenType.RPAREN else temp_i
                                return result, next_index, TYPE_INVALID

                           # Argument was valid, update index
                           i = arg_end_index

                      # Loop finished, check closing parenthesis
                      if i < len(tokens) and tokens[i].type == TokenType.RPAREN:
                           next_index = i + 1 # Consume ')'
                           # Determine result type based on reporter
                           inferred_type = TYPE_LIST if reporter_name == 'list' else TYPE_STRING if reporter_name == 'word' else TYPE_ANY
                      else:
                           result.add_error(ValidationError(f"Expected ')' to close '({reporter_name} ...)' opened on line {paren_token.line}", line_number=tokens[i-1].line if i > 0 else paren_token.line))
                           next_index = i
                           inferred_type = TYPE_INVALID

            # --- Standard Parenthesized Expression ---
            if not is_special_reporter:
                # Parse the expression inside parentheses.
                inner_result, inner_end_index, inner_type = self._validate_expression(tokens, i + 1, min_precedence=-1)
                result.merge(inner_result)
                inferred_type = inner_type # Type of parenthesized expr is type of inner expr

                # Check for the closing parenthesis.
                if inner_end_index < len(tokens) and tokens[inner_end_index].type == TokenType.RPAREN:
                    next_index = inner_end_index + 1 # Consume the RPAREN
                else:
                    expected_line = tokens[inner_end_index - 1].line if inner_end_index > 0 else current_token.line
                    result.add_error(ValidationError(
                        f"Expected ')' to close parenthesis opened on line {current_token.line}",
                        line_number=expected_line
                    ))
                    next_index = inner_end_index
                    if inferred_type != TYPE_INVALID:
                         inferred_type = TYPE_UNKNOWN

        # --- Handle Bracketed Command Blocks ---
        elif current_token.type == TokenType.LBRACKET:
            bracket_token = current_token
            # Validate structure, find matching ']'
            bracket_level = 1
            temp_i = i + 1
            while temp_i < len(tokens):
                if tokens[temp_i].type == TokenType.LBRACKET: bracket_level += 1
                elif tokens[temp_i].type == TokenType.RBRACKET:
                    bracket_level -= 1
                    if bracket_level == 0: break
                elif tokens[temp_i].type == TokenType.EOF: break
                temp_i += 1

            if bracket_level == 0:
                next_index = temp_i + 1 # Consume ']'
                inferred_type = TYPE_COMMAND_BLOCK
            else:
                result.add_error(ValidationError(
                    f"Expected ']' to close bracket opened on line {bracket_token.line}",
                    line_number=tokens[temp_i-1].line if temp_i > 0 else bracket_token.line
                ))
                next_index = temp_i
                inferred_type = TYPE_INVALID

        # --- Handle Reporters (excluding parenthesized 'list'/'word' and 0-arity handled as IDENTIFIER) ---
        elif current_token.type == TokenType.REPORTER:
            reporter_name = current_token.value.lower()
            arity = self.REPORTER_ARITY.get(reporter_name)

            # Disallow bare 'list' or 'word'
            if reporter_name in {'list', 'word'}:
                 result.add_error(ValidationError(
                     f"Unsupported syntax: Bare '{reporter_name}' reporter found. Use parenthesized '({reporter_name} ...)' form.",
                     line_number=current_token.line, code_snippet=current_token.value
                 ))
                 next_index = i + 1
                 inferred_type = TYPE_INVALID
            elif arity is None or arity == 0: # 0-arity handled as IDENTIFIER, None is error
                 # This case should ideally not be reached if 0-arity reporters are handled above
                 result.add_error(ValidationError(
                     f"Unknown or disallowed reporter '{current_token.value}' used in expression",
                     line_number=current_token.line, code_snippet=current_token.value
                 ))
                 next_index = i + 1
                 inferred_type = TYPE_INVALID
            else:
                 # Consume the reporter token itself
                 current_arg_index = i + 1
                 # Parse the expected number of arguments
                 for arg_num in range(arity):
                     if current_arg_index >= len(tokens) or tokens[current_arg_index].type == TokenType.EOF:
                         result.add_error(ValidationError(
                             f"Expected argument {arg_num + 1} for reporter '{reporter_name}', but found end of input",
                             line_number=current_token.line
                         ))
                         current_arg_index = len(tokens)
                         inferred_type = TYPE_INVALID # Mark as invalid due to missing args
                         break # Stop parsing args

                     # Parse the argument expression recursively, ignore type for now
                     arg_result, arg_end_index, _ = self._validate_expression(tokens, current_arg_index, min_precedence=-1)
                     result.merge(arg_result)

                     if not arg_result.is_valid:
                         if result.errors:
                             # Find the last error added by the merge and modify it
                             for err in reversed(result.errors):
                                  if f"Invalid argument {arg_num + 1}" not in err.message:
                                       err.message = f"Invalid argument {arg_num + 1} for '{reporter_name}': {err.message}"
                                       break
                         inferred_type = TYPE_INVALID # Mark as invalid due to bad arg

                     # Move to the start of the next potential argument
                     current_arg_index = arg_end_index
                     # If an arg was invalid, stop parsing args for this reporter
                     if inferred_type == TYPE_INVALID:
                          break

                 # After parsing args, the next_index is where the reporter call ends.
                 next_index = current_arg_index
                 # If no errors occurred, infer return type (basic for now)
                 if inferred_type != TYPE_INVALID:
                      # TODO: Use REPORTER_INFO for better type inference later
                      if reporter_name in {'random', 'random-float', 'sin', 'cos', 'tan', 'xcor', 'ycor', 'heading', 'distance', 'towards', 'count', 'length', 'abs'}:
                           inferred_type = TYPE_NUMBER
                      elif reporter_name in {'any?'}:
                           inferred_type = TYPE_BOOLEAN
                      elif reporter_name in {'item', 'position', 'min', 'max'}: # Can return num/bool/item-type
                           inferred_type = TYPE_ANY
                      elif reporter_name in {'in-radius'}:
                           inferred_type = TYPE_AGENTSET
                      else:
                           inferred_type = TYPE_ANY # Default for unknown reporters

        # --- Handle Unexpected Tokens ---
        else:
            result.add_error(ValidationError(
                f"Unexpected token when expecting an expression term or prefix operator: {current_token.type.name} ('{current_token.value}')",
                line_number=current_token.line, code_snippet=current_token.value
            ))
            next_index = i + 1 # Consume the unexpected token
            inferred_type = TYPE_INVALID

        return result, next_index, inferred_type


    def _validate_expression(self, tokens: List[Token], start_index: int, min_precedence: int = -1) -> Tuple[ValidationResult, int, str]:
        """
        Recursively validates a NetLogo expression using Pratt parsing (Top-Down Operator Precedence).
        Handles infix operators based on precedence and performs basic type checking.

        Args:
            tokens: The list of tokens.
            start_index: The index in the token list where the expression starts.
            min_precedence: The minimum precedence level for operators to be consumed.

        Returns:
            A tuple containing:
            - ValidationResult: Indicates if the parsed expression is valid.
            - int: The index of the token immediately *after* the parsed expression.
            - str: The inferred type string of the parsed expression.
        """
        result = ValidationResult(True)
        i = start_index

        # 1. Parse the left-hand side (prefix operators, primary terms)
        left_result, current_index, left_type = self._parse_prefix_or_primary(tokens, i)
        result.merge(left_result) # Merge validation result immediately

        # If the primary part is invalid or resulted in an invalid type, propagate
        if not result.is_valid or left_type == TYPE_INVALID:
            # Ensure current_index is advanced at least by 1 if parsing failed at start
            current_index = max(current_index, i + 1)
            return result, current_index, TYPE_INVALID

        # 2. Loop while the next token is an infix operator with sufficient precedence
        while True:
            if current_index >= len(tokens) or tokens[current_index].type == TokenType.EOF:
                break # End of input

            operator_token = tokens[current_index]
            current_precedence = self._get_token_precedence(operator_token)

            # Stop if token is not an operator or precedence is too low
            if current_precedence < min_precedence:
                break

            # --- Consume the operator ---
            current_index += 1 # Move past the operator

            # --- Parse the right-hand side ---
            # Adjust precedence for right operand based on associativity (assume left for now)
            next_min_precedence = current_precedence + 1
            # TODO: Handle right-associativity for '^' if needed.

            right_result, next_index, right_type = self._validate_expression(tokens, current_index, min_precedence=next_min_precedence)
            result.merge(right_result)

            # If the right side is invalid, propagate the invalid type and stop
            if not result.is_valid or right_type == TYPE_INVALID:
                 # Ensure next_index is advanced if right-side parsing failed immediately
                 next_index = max(next_index, current_index + 1)
                 return result, next_index, TYPE_INVALID

            # --- Perform Type Checking ---
            current_op_result_type = TYPE_UNKNOWN # Type of the result of this operation
            valid_operation = True

            # Define allowed types for operators (simplified)
            numeric_ops = {TokenType.OPERATOR} # +, -, *, /, ^
            string_ops = {TokenType.STRING_CONCAT} # ++
            boolean_ops = {TokenType.LOGICAL} # and, or (not is prefix)
            comparison_ops = {TokenType.COMPARISON} # =, !=, >, <, >=, <=

            # Types that can be operands for numeric ops
            allowed_numeric_operands = {TYPE_NUMBER, TYPE_ANY, TYPE_UNKNOWN}
            # Types that can be operands for string ops (NetLogo coerces numbers/bools)
            allowed_string_operands = {TYPE_STRING, TYPE_NUMBER, TYPE_BOOLEAN, TYPE_ANY, TYPE_UNKNOWN}
            # Types that can be operands for boolean ops
            allowed_boolean_operands = {TYPE_BOOLEAN, TYPE_ANY, TYPE_UNKNOWN}
            # Types for comparison (very permissive in NetLogo)
            allowed_comparison_operands = {TYPE_NUMBER, TYPE_STRING, TYPE_BOOLEAN, TYPE_LIST, TYPE_AGENT, TYPE_PATCH, TYPE_LINK, TYPE_ANY, TYPE_UNKNOWN}

            op_type = operator_token.type
            op_val = operator_token.value.lower() # Use lower for +, -, etc.

            if op_type in numeric_ops:
                if left_type not in allowed_numeric_operands or right_type not in allowed_numeric_operands:
                    # Error if definitely incompatible (e.g., STRING + NUMBER)
                    if TYPE_STRING in (left_type, right_type) or \
                       TYPE_BOOLEAN in (left_type, right_type) or \
                       TYPE_LIST in (left_type, right_type): # Add other non-numeric types
                        result.add_error(ValidationError(
                            f"Operator '{op_val}' expects numeric operands, but got {left_type} and {right_type}",
                            line_number=operator_token.line
                        ))
                        valid_operation = False
                # Result is NUMBER unless operands were ANY/UNKNOWN
                current_op_result_type = TYPE_NUMBER if left_type == TYPE_NUMBER and right_type == TYPE_NUMBER else TYPE_ANY

            elif op_type == TokenType.STRING_CONCAT:
                 # Check for the specific invalid case: NUMBER ++ NUMBER
                 if left_type == TYPE_NUMBER and right_type == TYPE_NUMBER:
                      result.add_error(ValidationError(
                          f"Operator '++' cannot be used with two number operands. Use 'word' or '+' instead.",
                          line_number=operator_token.line,
                          code_snippet=f"... {operator_token.value} ..." # Simple snippet
                      ))
                      valid_operation = False
                 # Check if types are generally disallowed (e.g., AGENTSET ++ LIST)
                 elif left_type not in allowed_string_operands or right_type not in allowed_string_operands:
                      result.add_error(ValidationError(
                          f"Operator '++' has incompatible operand types: {left_type} and {right_type}",
                          line_number=operator_token.line
                      ))
                      valid_operation = False
                 # Result of ++ is always string
                 current_op_result_type = TYPE_STRING

            elif op_type in boolean_ops and op_val in {'and', 'or'}:
                if left_type not in allowed_boolean_operands or right_type not in allowed_boolean_operands:
                     # Error if definitely incompatible
                     if TYPE_NUMBER in (left_type, right_type) or \
                        TYPE_STRING in (left_type, right_type) or \
                        TYPE_LIST in (left_type, right_type): # Add other non-boolean types
                         result.add_error(ValidationError(
                             f"Operator '{op_val}' expects boolean operands, but got {left_type} and {right_type}",
                             line_number=operator_token.line
                         ))
                         valid_operation = False
                # Result is BOOLEAN unless operands were ANY/UNKNOWN
                current_op_result_type = TYPE_BOOLEAN if left_type == TYPE_BOOLEAN and right_type == TYPE_BOOLEAN else TYPE_ANY

            elif op_type in comparison_ops:
                 # Comparisons are generally permissive type-wise in NetLogo
                 if left_type not in allowed_comparison_operands or right_type not in allowed_comparison_operands:
                      pass # Allow most comparisons for now
                 current_op_result_type = TYPE_BOOLEAN # Result is boolean
            else:
                 # Should not happen if _get_token_precedence is correct
                 result.add_error(ValidationError(f"Unhandled operator type: {op_type.name}", line_number=operator_token.line))
                 valid_operation = False


            # --- Update State ---
            if not valid_operation:
                 left_type = TYPE_INVALID # Mark the combined expression as invalid
            else:
                 # If either operand was ANY/UNKNOWN, the result type might also be uncertain,
                 # unless the operator guarantees a specific type (like comparisons -> BOOLEAN, ++ -> STRING)
                 if current_op_result_type not in {TYPE_BOOLEAN, TYPE_STRING}: # Operators with fixed result types
                      if left_type in {TYPE_ANY, TYPE_UNKNOWN} or right_type in {TYPE_ANY, TYPE_UNKNOWN}:
                           left_type = TYPE_ANY
                      else:
                           left_type = current_op_result_type # Use the determined type (e.g., NUMBER)
                 else:
                      left_type = current_op_result_type # Use the fixed result type (BOOLEAN or STRING)


            # Update the current index to after the right operand.
            current_index = next_index

            # If the operation resulted in an invalid type, stop processing further operators at this level
            if left_type == TYPE_INVALID:
                 break

        # Loop finished, return the final result, index, and inferred type
        return result, current_index, left_type

    # --- Control Structure Validators ---
    def _validate_if_statement(self, tokens: List[Token], start_idx: int) -> Tuple[ValidationResult, int]:
        """Validate if/ifelse/ifelse-value using tokens and the expression validator."""
        # Returns: ValidationResult, consumed_token_count
        result = ValidationResult(True)
        i = start_idx
        statement_token = tokens[i]
        statement_type = statement_token.value.lower()

        # Handle multi-conditional format: (ifelse cond1 [...] cond2 [...] [...])
        is_multi_conditional = False
        if i > 0 and tokens[i-1].type == TokenType.LPAREN:
            is_multi_conditional = True
            # The multi-conditional validator expects to start *at* the LPAREN
            return self._validate_multi_conditional(tokens, i - 1, statement_type)

        # --- Standard if/ifelse ---
        i += 1 # Move past 'if'/'ifelse'

        if i >= len(tokens) or tokens[i].type == TokenType.EOF:
            result.add_error(ValidationError(f"Incomplete {statement_type} - missing condition", line_number=statement_token.line))
            return result, i

        # Validate condition expression
        cond_result, cond_end_i, cond_type = self._validate_expression(tokens, i, min_precedence=-1)
        result.merge(cond_result)

        # Check condition type (should be boolean)
        # Allow ANY/UNKNOWN as condition might resolve at runtime
        if cond_type not in {TYPE_BOOLEAN, TYPE_ANY, TYPE_UNKNOWN, TYPE_INVALID}:
             result.add_error(ValidationError(
                 f"{statement_type} expects a boolean condition, but got {cond_type}",
                 line_number=tokens[i].line # Line where condition starts
             ))

        # Attempt recovery even if condition is invalid/wrong type
        if not cond_result.is_valid or cond_type == TYPE_INVALID:
             # Try to find the start of the true branch '[' for recovery
             found_bracket = False
             temp_i = cond_end_i
             while temp_i < len(tokens) and tokens[temp_i].type != TokenType.LBRACKET and tokens[temp_i].type != TokenType.EOF:
                 temp_i += 1
             if temp_i < len(tokens) and tokens[temp_i].type == TokenType.LBRACKET:
                 i = temp_i # Recovered position
                 found_bracket = True
             else:
                 i = cond_end_i # Could not recover, proceed from end of invalid condition
                 # Add error if recovery failed and no bracket found immediately after invalid condition
                 if i < len(tokens) and tokens[i].type != TokenType.LBRACKET:
                      result.add_error(ValidationError(f"Expected '[' after invalid condition in {statement_type}", line_number=tokens[i-1].line if i > 0 else statement_token.line))

        else: # Condition was valid
             i = cond_end_i

        # Expect true branch opening bracket
        if i >= len(tokens) or tokens[i].type != TokenType.LBRACKET:
            result.add_error(ValidationError(f"Expected '[' after condition in {statement_type}", line_number=tokens[i-1].line if i > 0 else statement_token.line))
            # If bracket is missing, we can't reliably parse the rest
            return result, i
        else:
            i += 1 # Move past '['
            true_branch_start_i = i
            # Find matching RBRACKET
            bracket_level = 1
            while i < len(tokens):
                if tokens[i].type == TokenType.LBRACKET: bracket_level += 1
                elif tokens[i].type == TokenType.RBRACKET:
                    bracket_level -= 1
                    if bracket_level == 0: break
                elif tokens[i].type == TokenType.EOF: bracket_level = -1; break
                i += 1

            if bracket_level != 0:
                result.add_error(ValidationError(f"Unclosed bracket in {statement_type} true branch", line_number=tokens[true_branch_start_i-1].line))
                true_branch_end_i = i
            else:
                true_branch_end_i = i
                i += 1 # Move past ']'

            # Validate true branch contents
            true_branch_tokens = tokens[true_branch_start_i:true_branch_end_i]
            true_branch_result, _ = self._validate_branch_contents( # Ignore branch type for now
                 true_branch_tokens,
                 statement_type,
                 opening_bracket_line=tokens[true_branch_start_i-1].line
            )
            result.merge(true_branch_result)

        # Handle false branch for ifelse/ifelse-value
        if statement_type in {'ifelse', 'ifelse-value'}:
            if i >= len(tokens) or tokens[i].type != TokenType.LBRACKET:
                result.add_error(ValidationError(f"Missing '[' for false branch in {statement_type}", line_number=tokens[i-1].line if i > 0 else statement_token.line))
                # If bracket is missing, we can't reliably parse the rest
                return result, i
            else:
                i += 1 # Move past '['
                false_branch_start_i = i
                bracket_level = 1
                while i < len(tokens):
                    if tokens[i].type == TokenType.LBRACKET: bracket_level += 1
                    elif tokens[i].type == TokenType.RBRACKET:
                        bracket_level -= 1
                        if bracket_level == 0: break
                    elif tokens[i].type == TokenType.EOF: bracket_level = -1; break
                    i += 1

                if bracket_level != 0:
                    result.add_error(ValidationError(f"Unclosed bracket in {statement_type} false branch", line_number=tokens[false_branch_start_i-1].line))
                    false_branch_end_i = i
                else:
                    false_branch_end_i = i
                    i += 1 # Move past ']'

                # Validate false branch contents
                false_branch_tokens = tokens[false_branch_start_i:false_branch_end_i]
                false_branch_result, _ = self._validate_branch_contents( # Ignore branch type for now
                     false_branch_tokens,
                     statement_type,
                     opening_bracket_line=tokens[false_branch_start_i-1].line
                )
                result.merge(false_branch_result)

                # TODO: For ifelse-value, could check if branch types are compatible

        # Return the result and the number of tokens consumed by the entire if/ifelse statement
        return result, i - start_idx

    def _validate_multi_conditional(self, tokens: List[Token], start_idx: int, statement_type: str) -> Tuple[ValidationResult, int]:
        """Validate multi-conditional ifelse/ifelse-value using tokens."""
        # Returns: ValidationResult, consumed_token_count
        result = ValidationResult(True)
        i = start_idx # Should start at LPAREN
        paren_token = tokens[i]

        if paren_token.type != TokenType.LPAREN:
             result.add_error(ValidationError(f"Expected '(' for multi-conditional {statement_type}", line_number=paren_token.line))
             return result, i + 1

        i += 1 # Move past '('

        if i >= len(tokens) or tokens[i].type != TokenType.COMMAND or tokens[i].value.lower() != statement_type:
             result.add_error(ValidationError(f"Expected '{statement_type}' after '('", line_number=paren_token.line))
             # Attempt recovery by finding RPAREN
             paren_level = 1
             while i < len(tokens):
                 if tokens[i].type == TokenType.LPAREN: paren_level += 1
                 elif tokens[i].type == TokenType.RPAREN:
                     paren_level -= 1
                     if paren_level == 0: break
                 elif tokens[i].type == TokenType.EOF: break
                 i += 1
             return result, i + 1 if i < len(tokens) else i
        i += 1 # Move past statement type (ifelse or ifelse-value)

        has_processed_at_least_one_pair = False
        while i < len(tokens):
            # --- Check for loop termination conditions first ---
            current_token = tokens[i]

            # 1. End of statement?
            if current_token.type == TokenType.RPAREN:
                i += 1 # Consume ')'
                break # Exit loop

            # 2. Optional final else block?
            if current_token.type == TokenType.LBRACKET:
                i += 1 # Consume '['
                else_branch_start_i = i
                bracket_level = 1
                while i < len(tokens):
                    if tokens[i].type == TokenType.LBRACKET: bracket_level += 1
                    elif tokens[i].type == TokenType.RBRACKET:
                        bracket_level -= 1
                        if bracket_level == 0: break
                    elif tokens[i].type == TokenType.EOF: bracket_level = -1; break
                    i += 1

                if bracket_level != 0:
                    result.add_error(ValidationError(f"Unclosed bracket in {statement_type} else branch", line_number=tokens[else_branch_start_i-1].line))
                    else_branch_end_i = i
                else:
                    else_branch_end_i = i
                    i += 1 # Move past ']'

                # Validate else branch contents
                else_branch_tokens = tokens[else_branch_start_i:else_branch_end_i]
                else_branch_result, _ = self._validate_branch_contents( # Ignore branch type
                     else_branch_tokens,
                     statement_type,
                     opening_bracket_line=tokens[else_branch_start_i-1].line
                )
                result.merge(else_branch_result)

                # After the final else branch, we MUST find the closing parenthesis
                if i >= len(tokens) or tokens[i].type != TokenType.RPAREN:
                     result.add_error(ValidationError(f"Expected ')' after final else branch in {statement_type}", line_number=tokens[i-1].line if i > 0 else paren_token.line))
                     # Don't consume if RPAREN is missing, let the final check handle it
                else:
                     i += 1 # Consume ')'
                break # Exit loop after processing final else

            # --- If not RPAREN or LBRACKET, assume it's a condition-branch pair ---
            has_processed_at_least_one_pair = True

            # 3. Parse Condition
            cond_result, cond_end_i, cond_type = self._validate_expression(tokens, i, min_precedence=-1)
            result.merge(cond_result)

            # Check condition type
            if cond_type not in {TYPE_BOOLEAN, TYPE_ANY, TYPE_UNKNOWN, TYPE_INVALID}:
                 result.add_error(ValidationError(
                     f"{statement_type} expects a boolean condition, but got {cond_type}",
                     line_number=tokens[i].line
                 ))

            # If condition is invalid, stop processing this statement
            if not cond_result.is_valid or cond_type == TYPE_INVALID:
                 i = cond_end_i # Advance index past the invalid expression
                 break # Exit the while loop

            # Condition was valid, update index
            i = cond_end_i

            # 4. Expect and Parse Command Block '[' ... ']'
            if i >= len(tokens) or tokens[i].type != TokenType.LBRACKET:
                 result.add_error(ValidationError(f"Expected '[' after condition in {statement_type}", line_number=tokens[i-1].line if i > 0 else paren_token.line))
                 # If '[' is missing, stop processing this statement
                 break # Exit the while loop
            else:
                 i += 1 # Move past '['
                 branch_start_i = i
                 bracket_level = 1
                 while i < len(tokens):
                     if tokens[i].type == TokenType.LBRACKET: bracket_level += 1
                     elif tokens[i].type == TokenType.RBRACKET:
                         bracket_level -= 1
                         if bracket_level == 0: break
                     elif tokens[i].type == TokenType.EOF: bracket_level = -1; break
                     i += 1

                 if bracket_level != 0:
                     result.add_error(ValidationError(f"Unclosed bracket in {statement_type} branch", line_number=tokens[branch_start_i-1].line))
                     branch_end_i = i
                     # Stop processing if branch is unclosed
                     break
                 else:
                     branch_end_i = i
                     i += 1 # Move past ']'

                 # Validate condition branch contents
                 branch_tokens = tokens[branch_start_i:branch_end_i]
                 branch_result, _ = self._validate_branch_contents( # Ignore branch type
                      branch_tokens,
                      statement_type,
                      opening_bracket_line=tokens[branch_start_i-1].line
                 )
                 result.merge(branch_result)
                 # Loop continues to check for RPAREN, LBRACKET, or next condition

        # --- Post-loop checks ---
        # Check if the loop exited because it reached the end of tokens unexpectedly
        if i < len(tokens) and tokens[i].type == TokenType.EOF and (i == 0 or tokens[i-1].type != TokenType.RPAREN):
             result.add_error(ValidationError(f"Unexpected end of input within multi-conditional {statement_type}", line_number=tokens[i-1].line if i > 0 else paren_token.line))
        # Check if we ended correctly (the token *before* the current index 'i' should be RPAREN)
        elif i == 0 or (i > 0 and tokens[i-1].type != TokenType.RPAREN):
             # Avoid adding duplicate error if already reported missing ')'
             if not result.errors or "Expected ')'" not in result.errors[-1].message:
                 line_num = tokens[i-1].line if i > 0 else paren_token.line
                 result.add_error(ValidationError(f"Expected ')' to end multi-conditional {statement_type}", line_number=line_num))
        # Check if any condition-branch pairs were processed
        elif not has_processed_at_least_one_pair and i > 0 and tokens[i-1].type == TokenType.RPAREN: # Ensure it didn't just parse '()'
             # Check if the token before RPAREN was the statement type
             if i > 1 and tokens[i-2].type == TokenType.COMMAND and tokens[i-2].value.lower() == statement_type:
                  result.add_error(ValidationError(f"Multi-conditional {statement_type} must have at least one condition/branch pair or an else branch", line_number=paren_token.line))


        # Return the result and the number of tokens consumed by the entire multi-conditional statement
        return result, i - start_idx

    def _validate_branch_contents(self, tokens: List[Token], statement_type: str, opening_bracket_line: int) -> Tuple[ValidationResult, str]:
        """
        Validate the contents of an if/ifelse/ifelse-value branch using tokens.
        Returns the validation result and the inferred type (for ifelse-value).
        """
        result = ValidationResult(True)
        branch_start_line = tokens[0].line if tokens else opening_bracket_line
        branch_type = TYPE_UNKNOWN

        if not tokens:
            # Empty command blocks are okay. Empty reporter blocks are runtime errors.
            if statement_type == 'ifelse-value':
                 branch_type = TYPE_UNKNOWN # Syntactically okay, but type is unknown
            else:
                 branch_type = TYPE_COMMAND_BLOCK # Valid empty command sequence
            return result, branch_type

        if statement_type == 'ifelse-value':
            # Expect exactly one valid expression that consumes all tokens
            expr_result, end_idx, expr_type = self._validate_expression(tokens, 0, min_precedence=-1)
            result.merge(expr_result)
            branch_type = expr_type # Type of the branch is the type of the expression

            if not expr_result.is_valid:
                 # Add context if the error message doesn't already have it
                 if result.errors and "ifelse-value branch" not in result.errors[-1].message:
                      # Find the last error added by the merge and modify it
                      for err in reversed(result.errors):
                           if "ifelse-value branch" not in err.message:
                                err.message = f"Invalid expression in ifelse-value branch: {err.message}"
                                break
                 branch_type = TYPE_INVALID
            elif end_idx != len(tokens):
                 # Expression was valid but didn't consume the whole block
                 result.add_error(ValidationError(
                     f"Unexpected token '{tokens[end_idx].value}' after expression in ifelse-value branch. Branch should contain only one expression.",
                     line_number=tokens[end_idx].line
                 ))
                 branch_type = TYPE_INVALID # Mark as invalid due to extra tokens
            # If valid and consumed all, branch_type remains expr_type.
        else: # 'if' or 'ifelse' command block validation
            # Use the main syntax checker for the branch content
            eof_token = Token(TokenType.EOF, '', tokens[-1].line, tokens[-1].column + 1) if tokens else Token(TokenType.EOF, '', branch_start_line, 1)
            # Validate the sequence of commands within the block
            branch_syntax_result = self._check_syntax_tokenized(tokens + [eof_token])

            if not branch_syntax_result.is_valid:
                 # Add context to errors from the branch validation
                 for error in branch_syntax_result.errors:
                      error.message = f"Invalid command sequence in {statement_type} branch (line ~{branch_start_line}): {error.message}"
                      # Adjust line numbers if possible? Difficult without original code mapping.
                 result.merge(branch_syntax_result)
                 branch_type = TYPE_INVALID
            else:
                 branch_type = TYPE_COMMAND_BLOCK # Valid command sequence

        return result, branch_type

    def _validate_command(self, tokens: List[Token], start_idx: int) -> Tuple[ValidationResult, int]:
        """Validate a command and its arguments using tokens."""
        # Returns: ValidationResult, consumed_token_count
        result = ValidationResult(True)
        i = start_idx
        command_token = tokens[i]
        command_lower = command_token.value.lower()

        if command_token.type != TokenType.COMMAND or command_lower not in self.allowed_commands:
             # This case should not be reached if called from _check_syntax_tokenized
             result.add_error(ValidationError(f"Internal error: _validate_command called with non-command token {command_token.value}", line_number=command_token.line))
             return result, i + 1

        i += 1 # Move past command

        num_expected_args = 0
        expected_arg_types = [] # List to hold expected types for each arg
        if command_lower in {'fd', 'forward', 'bk', 'back', 'rt', 'right', 'lt', 'left'}:
            num_expected_args = 1
            expected_arg_types = [TYPE_NUMBER]
        elif command_lower == 'set':
            num_expected_args = 2
            expected_arg_types = [TYPE_ANY, TYPE_ANY] # Variable name (checked separately), value
        elif command_lower == 'let':
             num_expected_args = 2
             expected_arg_types = [TYPE_ANY, TYPE_ANY] # Variable name (checked separately), value
        elif command_lower == 'stop':
             num_expected_args = 0
             expected_arg_types = []


        consumed_args = 0
        arg_start_index = i
        while consumed_args < num_expected_args:
            arg_start_index = i # Track start of current argument parse
            if i >= len(tokens) or tokens[i].type == TokenType.EOF:
                 result.add_error(ValidationError(f"Command '{command_lower}' expects {num_expected_args} arg(s), found end of input", line_number=command_token.line))
                 break

            # Special handling for 'set'/'let' variable name (first argument)
            if command_lower in {'set', 'let'} and consumed_args == 0:
                 var_token = tokens[i]
                 if var_token.type != TokenType.IDENTIFIER:
                      result.add_error(ValidationError(f"Expected variable name after '{command_lower}', found {var_token.type.name}", line_number=var_token.line))
                 # Basic check for valid identifier format (already done by tokenizer, but good safeguard)
                 elif not re.match(r'^[a-zA-Z][\w\-]*$', var_token.value):
                      result.add_error(ValidationError(f"Invalid variable name format: '{var_token.value}'", line_number=var_token.line))
                 # Check if it's a disallowed primitive name (e.g., cannot 'set fd [...]')
                 elif var_token.value.lower() in self.allowed_commands or var_token.value.lower() in self.allowed_reporters:
                      result.add_error(ValidationError(f"Cannot use command/reporter name '{var_token.value}' as a variable", line_number=var_token.line))

                 # Consume only the variable name token
                 next_i = i + 1
                 arg_type = TYPE_ANY # Variable name itself doesn't have a type relevant here
            else:
                 # Validate the argument expression
                 arg_result, next_i, arg_type = self._validate_expression(tokens, i, min_precedence=-1)
                 result.merge(arg_result)

                 # Add context to the error message if invalid
                 if not arg_result.is_valid:
                      if result.errors:
                           # Find the last error added by the merge and modify it
                           for err in reversed(result.errors):
                                if f"Invalid arg {consumed_args + 1}" not in err.message:
                                     err.message = f"Invalid arg {consumed_args + 1} for '{command_lower}': {err.message}"
                                     break

                 # --- Argument Type Check ---
                 current_expected_type = expected_arg_types[consumed_args]
                 # Allow ANY/UNKNOWN/INVALID to pass type check here, focus on definite mismatches
                 if current_expected_type != TYPE_ANY and \
                    arg_type not in {current_expected_type, TYPE_ANY, TYPE_UNKNOWN, TYPE_INVALID}:
                      result.add_error(ValidationError(
                           f"Command '{command_lower}' expects arg {consumed_args + 1} to be {current_expected_type}, but got {arg_type}",
                           line_number=tokens[arg_start_index].line # Line where arg starts
                      ))

            # Update the loop index 'i' to point after the consumed argument
            i = next_i
            consumed_args += 1

            # If the argument parsing failed severely and didn't advance the index, break
            if i == arg_start_index and not result.is_valid:
                 # Add a generic error if not already present from _validate_expression
                 if not any(e.line_number == tokens[arg_start_index].line for e in result.errors):
                      result.add_error(ValidationError(f"Failed to parse argument {consumed_args} for '{command_lower}'", line_number=tokens[i].line))
                 break

        # Check if too many arguments were provided (if next token isn't EOF or another command/closing bracket)
        if result.is_valid and i < len(tokens) and tokens[i].type != TokenType.EOF:
             # Heuristic: If the next token could start another expression or command, assume too many args
             if self._can_start_expression(tokens[i]) or tokens[i].type in {TokenType.COMMAND, TokenType.LBRACKET, TokenType.LPAREN}:
                  # This check might be too aggressive, needs refinement.
                  # Consider context: are we inside brackets?
                  # For now, only flag if obviously wrong.
                  # Example: `fd 1 2` -> error
                  # Example: `fd 1 rt 90` -> okay
                  # Let's assume for now that if _validate_expression consumed correctly,
                  # the next token should ideally be EOF or something structural.
                  # This needs more robust handling based on context (inside block vs top level).
                  # Temporarily disable this check as it might be too noisy.
                  # result.add_error(ValidationError(f"Unexpected token '{tokens[i].value}' after arguments for command '{command_lower}'", line_number=tokens[i].line))
                  pass


        # Return the result and the *number of tokens consumed* by this command and its args
        return result, i - start_idx

    # --- Value Range Validator (Tokenized) ---
    def _check_value_ranges(self, tokens: List[Token]) -> ValidationResult:
        """
        Validate numeric value ranges using the token stream. Should be called after
        basic syntax validation to ensure tokens are meaningful.

        Args:
            tokens: The list of tokens representing the code.

        Returns:
            ValidationResult with any range violations.
        """
        result = ValidationResult(True)

        for token in tokens:
            if token.type == TokenType.NUMBER:
                num_str = token.value
                try:
                    value = float(num_str)
                    # Check against configured limits
                    if value > self.max_value:
                        result.add_error(ValidationError(
                            f"Value too large: {value} (maximum allowed: {self.max_value})",
                            line_number=token.line,
                            code_snippet=num_str
                        ))
                    if value < self.min_value:
                        result.add_error(ValidationError(
                            f"Value too small: {value} (minimum allowed: {self.min_value})",
                            line_number=token.line,
                            code_snippet=num_str
                        ))
                except ValueError:
                    # This shouldn't happen if the tokenizer is correct, but safeguard
                    result.add_error(ValidationError(
                        f"Invalid numeric token value: {num_str}",
                        line_number=token.line,
                        code_snippet=num_str
                    ))

        return result

    def measure_complexity(self, code: str) -> CodeComplexity:
        """
        Measure the complexity of NetLogo code.

        Args:
            code: The NetLogo code to analyze

        Returns:
            CodeComplexity: Enum value representing complexity level
        """
        # Clean the code
        code = re.sub(self.comment_pattern, '', code)
        code = code.lower()

        # Count various complexity indicators
        complexity_score = 0

        # 1. Basic movement commands (+1)
        if re.search(r'\b(fd|forward|bk|back|rt|right|lt|left)\b', code):
            complexity_score += 1

        # 2. Conditional statements (+1 per type)
        if re.search(r'\bif\b', code):
            complexity_score += 1
        if re.search(r'\bifelse\b', code):
            complexity_score += 1

        # 3. Variable usage (+1)
        if re.search(r'\b(set|let)\b', code):
            complexity_score += 1

        # 4. Advanced movement (+1)
        if re.search(r'\b(towards|distance|in-radius)\b', code):
            complexity_score += 1

        # 5. Random usage (+1)
        if re.search(r'\b(random|random-float)\b', code):
            complexity_score += 1

        # 6. Math functions (+1)
        if re.search(r'\b(sin|cos|tan)\b', code):
            complexity_score += 1

        # 7. Nested conditionals (+1)
        if re.search(r'\bif.*\[.*if\b', code) or re.search(r'\bifelse.*\[.*if\b', code):
            complexity_score += 1

        # Map score to complexity level, capping at the highest level
        if complexity_score <= 1:
            return CodeComplexity.SIMPLE
        elif complexity_score == 2:
            return CodeComplexity.BASIC
        elif complexity_score == 3:
            return CodeComplexity.MODERATE
        elif complexity_score == 4:
            return CodeComplexity.ADVANCED
        elif complexity_score == 5:
            return CodeComplexity.COMPLEX
        elif complexity_score == 6:
            return CodeComplexity.SOPHISTICATED
        else:
            return CodeComplexity.EXPERT

# Note: The test_verifier() function and its associated test cases
# have been moved to the separate test_verifier.py file for better organization.
